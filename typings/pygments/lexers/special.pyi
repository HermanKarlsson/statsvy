"""This type stub file was generated by pyright."""

from pygments.lexer import Lexer

"""
    pygments.lexers.special
    ~~~~~~~~~~~~~~~~~~~~~~~

    Special lexers.

    :copyright: Copyright 2006-2025 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""
__all__ = ["OutputLexer", "RawTokenLexer", "TextLexer"]

class TextLexer(Lexer):
    """ "Null" lexer, doesn't highlight anything."""

    name = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    url = ...
    version_added = ...
    priority = ...
    def get_tokens_unprocessed(
        self, text
    ): -> Generator[tuple[Literal[0], _TokenType, str], Any, None]:
        ...
    def analyse_text(text): -> float:
        ...

class OutputLexer(Lexer):
    """Simple lexer that highlights everything as ``Token.Generic.Output``."""

    name = ...
    aliases = ...
    url = ...
    version_added = ...
    _example = ...
    def get_tokens_unprocessed(
        self, text
    ): -> Generator[tuple[Literal[0], _TokenType, str], Any, None]:
        ...

_ttype_cache = ...

class RawTokenLexer(Lexer):
    """Recreate a token stream formatted with the `RawTokenFormatter`.

    Additional options accepted:

    `compress`
        If set to ``"gz"`` or ``"bz2"``, decompress the token stream with
        the given compression algorithm before lexing (default: ``""``).
    """

    name = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    url = ...
    version_added = ...
    def __init__(self, **options) -> None: ...
    def get_tokens(
        self, text
    ): -> Generator[tuple[_TokenType, str | Any] | tuple[_TokenType | Any, str | Any], Any, None]:
        ...
    def get_tokens_unprocessed(
        self, text
    ): -> Generator[tuple[int, _TokenType | Any, str | Any], Any, None]:
        ...
